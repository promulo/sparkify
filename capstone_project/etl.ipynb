{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, udf\n",
    "from pyspark.sql.types import IntegerType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "input_data = \"s3a://paulo-dend/data\"\n",
    "output_data = \"s3a://paulo-dend/tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf(IntegerType())\n",
    "def get_date_part(date_delta, key):\n",
    "    date = datetime(year=1960, month=1, day=1) + timedelta(days=int(date_delta))\n",
    "    parts = {\n",
    "        \"day\": date.day,\n",
    "        \"weekday\": date.isoweekday(),\n",
    "        \"month\": date.month,\n",
    "        \"year\": date.year\n",
    "    }\n",
    "    return parts[key]\n",
    "\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"com.github.saurfang.sas.spark\") \\\n",
    "    .load(f\"{input_data}/i94_apr16_sub.sas7bdat\")\n",
    "\n",
    "date_parts = [\"day\", \"weekday\", \"month\", \"year\"]\n",
    "for part in date_parts:\n",
    "    df = df.withColumn(part, get_date_part(\"arrdate\", lit(part)))\n",
    "\n",
    "df.createOrReplaceTempView(\"arrivals\")\n",
    "\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT arrdate, day, weekday, month, year FROM arrivals\n",
    "\"\"\")\n",
    "time_table \\\n",
    "    .write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{output_data}/time.parquet\")\n",
    "\n",
    "people_table = df.select(\n",
    "    df.admnum.cast(LongType()).alias(\"admission_number\"),\n",
    "    df.biryear.cast(IntegerType()).alias(\"birth_year\"),\n",
    "    df.gender,\n",
    "    df.i94addr.alias(\"address_state\")\n",
    ")\n",
    "people_table \\\n",
    "    .write \\\n",
    "    .partitionBy(\"gender\", \"birth_year\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{output_data}/people.parquet\")\n",
    "\n",
    "visas_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT visatype AS type,\n",
    "    CASE\n",
    "        WHEN i94visa = 1.0 THEN 'business'\n",
    "        WHEN i94visa = 2.0 THEN 'leisure'\n",
    "        WHEN i94visa = 3.0 THEN 'student'\n",
    "    END AS purpose\n",
    "    FROM arrivals\n",
    "\"\"\")\n",
    "visas_table \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{output_data}/visas.parquet\")\n",
    "\n",
    "arrivals_table = df.select(\n",
    "    df.cicid.cast(LongType()).alias(\"id\"),\n",
    "    df.admnum.cast(LongType()).alias(\"person_id\"),\n",
    "    df.visatype.alias(\"visa\"),\n",
    "    df.i94res.cast(IntegerType()).alias(\"country_of_origin\"),\n",
    "    df.i94port.alias(\"port_of_entry\"),\n",
    "    df.arrdate.cast(LongType()).alias(\"date\"),\n",
    "    df.airline,\n",
    "    df.fltno.alias(\"flight_number\"),\n",
    "    df.dtaddto.alias(\"allowed_until_date\"),\n",
    "    df.visapost.alias(\"visa_issuer\")\n",
    ")\n",
    "arrivals_table \\\n",
    "    .write \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{output_data}/arrivals.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "US_STATES = (\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\",\n",
    "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\",\n",
    "    \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\",\n",
    "    \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\",\n",
    "    \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"\n",
    ")\n",
    "\n",
    "df = spark.read.csv(f\"{input_data}/i94_ports_all.csv\", header=True).dropna()\n",
    "df.createOrReplaceTempView(\"ports_raw\")\n",
    "\n",
    "ports_of_entry_table = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT i94_port_code AS code, i94_port_name AS name, i94_port_state AS state\n",
    "    FROM ports_raw\n",
    "    WHERE i94_port_state IN {US_STATES}\n",
    "\"\"\")\n",
    "ports_of_entry_table \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{output_data}/ports_of_entry.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(f\"{input_data}/i94_countries_all.csv\", header=True).dropna()\n",
    "\n",
    "df = df.where(\n",
    "    ~(df.i94_country_name.startswith('No Country Code')\n",
    "      | df.i94_country_name.startswith('Collapsed'))\n",
    ")\n",
    "df = df.withColumn(\"country_code_as_int\", df.i94_country_code.cast(IntegerType()))\n",
    "\n",
    "remove_prefix_udf = udf(lambda value, prefix: value.replace(prefix, ''))\n",
    "df = df.withColumn(\n",
    "    \"country_name_without_prefix\", remove_prefix_udf(\"i94_country_name\", lit(\"INVALID: \"))\n",
    ")\n",
    "\n",
    "countries_table = df.select(\n",
    "    df.country_code_as_int.alias(\"i94_code\"),\n",
    "    df.country_name_without_prefix.alias(\"name\")\n",
    ")\n",
    "countries_table \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{output_data}/countries.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ports = spark.read.parquet(f\"{output_data}/ports_of_entry.parquet\")\n",
    "print(ports.count())\n",
    "ports.where(ports.code == \"NYC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries = spark.read.parquet(f\"{output_data}/countries.parquet\")\n",
    "print(countries.count())\n",
    "countries.where(countries.name == \"BRAZIL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "arrivals = spark.read.parquet(\"out/arrivals.parquet\")\n",
    "print(arrivals.count())\n",
    "arrivals.show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
