{
 "cells": [
  {
   "source": [
    "# Part I: ETL pipeline for processing of the CSV data files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import cassandra\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the ETL process is to compile a list of all the raw CSV files inside the `event_data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = Path.cwd() / 'event_data'\n",
    "for root, _, _ in os.walk(dataset_path):    \n",
    "    dataset_files = sorted(Path(root).glob('*.csv'))"
   ]
  },
  {
   "source": [
    "The next step is to process the above listed files in order to create the single CSV that will later be used to populate the Apache Casssandra tables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_CSV_FILE = 'event_datafile_new.csv'\n",
    "\n",
    "full_data_rows_list = []\n",
    "for f in dataset_files:\n",
    "    with open(f, 'r', encoding='utf8', newline='') as csv_file: \n",
    "        csv_reader = csv.reader(csv_file)         \n",
    "        next(csv_reader) # skips CSV header line\n",
    "        for line in csv_reader:\n",
    "            full_data_rows_list.append(line) \n",
    "\n",
    "csv.register_dialect('events', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "# here we join all valid rows from the separate CSVs into one single CSV file\n",
    "with open(DATA_CSV_FILE, 'w', encoding='utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='events')\n",
    "    writer.writerow([\n",
    "        'artist',\n",
    "        'firstName',\n",
    "        'gender',\n",
    "        'itemInSession',\n",
    "        'lastName',\n",
    "        'length',\n",
    "        'level',\n",
    "        'location',\n",
    "        'sessionId',\n",
    "        'song',\n",
    "        'userId'\n",
    "    ])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] != ''):\n",
    "            writer.writerow((\n",
    "                row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "source": [
    "Counting rows from the final CSV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_CSV_FILE, 'r', encoding='utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Data modelling using Apache Cassandra\n",
    "At this point, all CSV files from the `event_data` directory were processed and joined into a single CSV data file named `event_datafile_new.csv`. That file contains the following columns (the meaning of each one is detailed in parenthesis):\n",
    "- `artist` (artist name)\n",
    "- `firstName` (user first name)\n",
    "- `gender` (user gender)\n",
    "- `itemInSession` (session item number)\n",
    "- `lastName` (user last name)\n",
    "- `length` (song length)\n",
    "- `level` (user level, i.e. free or paid plan)\n",
    "- `location` (user location)\n",
    "- `sessionId` (session ID)\n",
    "- `song` (song title)\n",
    "- `userId` (user ID)\n",
    "\n",
    "The image below is a screenshot of what the data should look like in after the CSV processing code is executed:\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster()\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f25fb6b4940>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "create_keyspace_query = \"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS sparkify\n",
    "    WITH REPLICATION = { \n",
    "        'class': 'SimpleStrategy', \n",
    "        'replication_factor': 1 \n",
    "    };\n",
    "\"\"\"\n",
    "session.execute(create_keyspace_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('sparkify')\n"
   ]
  },
  {
   "source": [
    "## Data model and queries\n",
    "In the context of Sparkify's songplay events database, here are some of the questions that must be answered by our data model:\n",
    "\n",
    "1. Which were the artist name, song title and song length heard during the session with ID 338 and session item number 4?\n",
    "2. What is the full name of the user, the artist and song (sorted by session item number) listened by the user with ID 10 during the session with ID 182?\n",
    "3. What is the full name of all the users who listened to the song named 'All Hands Against His Own'?\n",
    "\n",
    "All tables were designed with the above questions in mind. The code that creates those tables in Apache Cassandra can be seen in the following cells.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f25f0426820>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "create_table_songplays_by_session = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songplay_events_by_session (\n",
    "    session_id INT,\n",
    "    session_item INT,\n",
    "    artist_name TEXT,\n",
    "    song_title TEXT,\n",
    "    song_length FLOAT,\n",
    "    user_id INT,\n",
    "    user_first_name TEXT,\n",
    "    user_last_name TEXT,\n",
    "    user_gender TEXT,\n",
    "    user_location TEXT,\n",
    "    user_plan TEXT,\n",
    "    PRIMARY KEY ((session_id), session_item, user_id)\n",
    ");\n",
    "\"\"\"\n",
    "session.execute(create_table_songplays_by_session)\n",
    "\n",
    "create_table_songplays_by_user = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songplay_events_by_user (\n",
    "    session_id INT,\n",
    "    session_item INT,\n",
    "    artist_name TEXT,\n",
    "    song_title TEXT,\n",
    "    song_length FLOAT,\n",
    "    user_id INT,\n",
    "    user_full_name TEXT,\n",
    "    user_gender TEXT,\n",
    "    user_location TEXT,\n",
    "    user_plan TEXT,\n",
    "    PRIMARY KEY ((user_id), session_id, session_item)\n",
    ");\n",
    "\"\"\"\n",
    "session.execute(create_table_songplays_by_user)\n",
    "\n",
    "create_table_songplays_by_song = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS songplay_events_by_song (\n",
    "    session_id INT,\n",
    "    session_item INT,\n",
    "    artist_name TEXT,\n",
    "    song_title TEXT,\n",
    "    song_length FLOAT,\n",
    "    user_id INT,\n",
    "    user_full_name TEXT,\n",
    "    user_gender TEXT,\n",
    "    user_location TEXT,\n",
    "    user_plan TEXT,\n",
    "    PRIMARY KEY ((song_title), user_id, session_id, session_item)\n",
    ");\n",
    "\"\"\"\n",
    "session.execute(create_table_songplays_by_song)"
   ]
  },
  {
   "source": [
    "With the tables created, the data previously compiled in the `event_datafile_new.csv` file can be used to populate them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "insert_query = \"\"\"\n",
    "INSERT INTO songplay_events_by_session (\n",
    "    session_id, \n",
    "    session_item, \n",
    "    artist_name, \n",
    "    song_title, \n",
    "    song_length, \n",
    "    user_id, \n",
    "    user_first_name, \n",
    "    user_last_name, \n",
    "    user_gender, \n",
    "    user_location, \n",
    "    user_plan\n",
    ")\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "\"\"\"\n",
    "insert_stmt_songplays_by_session = session.prepare(insert_query)\n",
    "\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO {} (\n",
    "    session_id, \n",
    "    session_item, \n",
    "    artist_name, \n",
    "    song_title, \n",
    "    song_length, \n",
    "    user_id, \n",
    "    user_full_name, \n",
    "    user_gender, \n",
    "    user_location, \n",
    "    user_plan\n",
    ")\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "\"\"\"\n",
    "insert_stmt_songplays_by_user = session.prepare(insert_query.format('songplay_events_by_user'))\n",
    "insert_stmt_songplays_by_song = session.prepare(insert_query.format('songplay_events_by_song'))\n",
    "\n",
    "with open(DATA_CSV_FILE, encoding='utf8') as f:\n",
    "    csv_reader = csv.reader(f)\n",
    "    next(csv_reader) # skips CSV header line\n",
    "    for line in csv_reader:\n",
    "        row = [\n",
    "            int(line[8]),   # session_id\n",
    "            int(line[3]),   # session_item\n",
    "            line[0],        # artist_name\n",
    "            line[9],        # song_title\n",
    "            float(line[5]), # song_length\n",
    "            int(line[10]),  # user_id \n",
    "            line[1],        # user_first_name\n",
    "            line[4],        # user_last_name\n",
    "            line[2],        # user_gender\n",
    "            line[7],        # user_location\n",
    "            line[6]         # user_plan\n",
    "        ]\n",
    "        session.execute(insert_stmt_songplays_by_session, row)\n",
    "        # tables 02 and 03 have a slightly different schema\n",
    "        row = [\n",
    "            int(line[8]),                                     # session_id\n",
    "            int(line[3]),                                     # session_item\n",
    "            line[0],                                          # artist_name\n",
    "            line[9],                                          # song_title\n",
    "            float(line[5]),                                   # song_length\n",
    "            int(line[10]),                                    # user_id\n",
    "            '{} {}'.format(line[1].strip(), line[4].strip()), # user_full_name\n",
    "            line[2],                                          # user_gender\n",
    "            line[7],                                          # user_location\n",
    "            line[6]                                           # user_plan\n",
    "        ]\n",
    "        session.execute(insert_stmt_songplays_by_user, row)\n",
    "        session.execute(insert_stmt_songplays_by_song, row)"
   ]
  },
  {
   "source": [
    "The following cells translate the questions to be answered by the data into Cassandra CQL queries. The numbering on the variables maps to the number given to the example questions above (i.e. `query_01` below relates to the question 1 defined and so on)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "artist: Faithless\nsong: Music Matters (Mark Knight Dub)\nlength: 495.31\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Which were the artist name, song title and song length heard during \n",
    "#    the session with ID 338 and session item number 4?\n",
    "query_01 = \"\"\"\n",
    "SELECT artist_name, song_title, song_length\n",
    "FROM songplay_events_by_session\n",
    "WHERE session_id = ? AND session_item = ?;\n",
    "\"\"\"\n",
    "query_01_stmt = session.prepare(query_01)\n",
    "result = session.execute(query_01_stmt, [338, 4])\n",
    "row = result.one()\n",
    "print('artist: {}\\nsong: {}\\nlength: {:.2f}\\n'.format(row.artist_name, row.song_title, row.song_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "artist: Down To The Bone\nsong: Keep On Keepin' On\nuser: Sylvie Cruz\n\nartist: Three Drives\nsong: Greece 2000\nuser: Sylvie Cruz\n\nartist: Sebastien Tellier\nsong: Kilometer\nuser: Sylvie Cruz\n\nartist: Lonnie Gordon\nsong: Catch You Baby (Steve Pitron & Max Sanna Radio Edit)\nuser: Sylvie Cruz\n\n"
     ]
    }
   ],
   "source": [
    "# 2. What is the full name of the user, the artist and song (sorted by session item number) \n",
    "#    listened by the user with ID 10 during the session with ID 182?\n",
    "query_02 = \"\"\"\n",
    "SELECT artist_name, song_title, user_full_name\n",
    "FROM songplay_events_by_user\n",
    "WHERE user_id = ? AND session_id = ?;\n",
    "\"\"\"\n",
    "query_02_stmt = session.prepare(query_02)\n",
    "result = session.execute(query_02_stmt, [10, 182])\n",
    "rows = result.all()\n",
    "for row in rows:\n",
    "    print('artist: {}\\nsong: {}\\nuser: {}\\n'.format(row.artist_name, row.song_title, row.user_full_name))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jacqueline Lynch\n\nTegan Levine\n\nSara Johnson\n\n"
     ]
    }
   ],
   "source": [
    "# 3. What is the full name of all the users who listened to the song named 'All Hands Against His Own'?\n",
    "query_03 = \"\"\"\n",
    "SELECT user_full_name \n",
    "FROM songplay_events_by_song\n",
    "WHERE song_title = ?;\n",
    "\"\"\"\n",
    "query_03_stmt = session.prepare(query_03)\n",
    "result = session.execute(query_03_stmt, ['All Hands Against His Own'])\n",
    "rows = result.all()\n",
    "for row in rows:\n",
    "    print('{}\\n'.format(row.user_full_name)) "
   ]
  },
  {
   "source": [
    "The code below is an alternative implementation of the `query_03` above which uses the concept of materialized views that is available in Apache Cassandra."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f25f145a910>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "create_view = \"\"\"\n",
    "CREATE MATERIALIZED VIEW IF NOT EXISTS songplay_events_by_song_mv AS \n",
    "    SELECT song_title, artist_name, user_id, user_full_name, session_id, session_item \n",
    "    FROM songplay_events_by_user \n",
    "    WHERE song_title IS NOT NULL AND session_id IS NOT NULL AND session_item IS NOT NULL AND user_id IS NOT NULL\n",
    "    PRIMARY KEY ((song_title), user_id, session_id, session_item);\n",
    "\"\"\"\n",
    "session.execute(create_view)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Jacqueline Lynch\n\nTegan Levine\n\nSara Johnson\n\n"
     ]
    }
   ],
   "source": [
    "# 3. What is the full name of all the users who listened to the song named 'All Hands Against His Own'?\n",
    "query_03_alt = \"\"\"\n",
    "SELECT user_full_name \n",
    "FROM songplay_events_by_song_mv\n",
    "WHERE song_title = ?;\n",
    "\"\"\"\n",
    "query_03_stmt = session.prepare(query_03_alt)\n",
    "result = session.execute(query_03_stmt, ['All Hands Against His Own'])\n",
    "rows = result.all()\n",
    "for row in rows:\n",
    "    print('{}\\n'.format(row.user_full_name)) "
   ]
  },
  {
   "source": [
    "After the ETL process and database querying is complete, the tables can be dropped from Cassandra."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_stmt = [\n",
    "    'DROP TABLE IF EXISTS songplay_events_by_session;',    \n",
    "    'DROP TABLE IF EXISTS songplay_events_by_song;',\n",
    "    'DROP MATERIALIZED VIEW IF EXISTS songplay_events_by_song_mv;',\n",
    "    'DROP TABLE IF EXISTS songplay_events_by_user;',\n",
    "    'DROP KEYSPACE IF EXISTS sparkify;'\n",
    "]\n",
    "for stmt in drop_stmt:\n",
    "    session.execute(stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}